{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's begin! \n",
    "### First we import some useful python libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "from nilearn import datasets\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../data\"\n",
    "\n",
    "pheno_data_tsv = f\"{data_dir}/participants.tsv\"\n",
    "brain_data_tsv = f\"{data_dir}/abide_nbsub-100_atlas-ho_meas-correlation_relmat.tsv\"\n",
    "\n",
    "\n",
    "pheno_df = pd.read_csv(pheno_data_tsv, sep=\"\\t\", index_col=0)\n",
    "brain_df = pd.read_csv(brain_data_tsv, sep=\"\\t\", index_col=0)\n",
    "\n",
    "pheno_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML-ready data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "X = brain_df.values\n",
    "\n",
    "# output\n",
    "outcome = \"DX_GROUP\"\n",
    "y = pheno_df[outcome]\n",
    "y_counts = y.value_counts()\n",
    "\n",
    "print(f\"Unique output clasess:\\n{y_counts}\")\n",
    "\n",
    "# Encode labels to integer categories\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train-test split\n",
    "- 80/20 ratio\n",
    "- Stratify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_subset_fraction = 0.2  #\n",
    "stratification = y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,  # input features\n",
    "    y,  # output labels\n",
    "    test_size=test_subset_fraction,\n",
    "    shuffle=True,  # shuffle dataset\n",
    "    # before splitting\n",
    "    stratify=stratification,\n",
    "    random_state=123,  # same shuffle each time\n",
    ")\n",
    "\n",
    "# print the size of our training and test groups\n",
    "print(\"training:\", len(X_train), \"testing:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okay finally, let's train your first model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = \"LR\"  # 'LR' or 'RF'\n",
    "\n",
    "if model == \"RF\":\n",
    "    clf = RandomForestClassifier(max_depth=3, class_weight=\"balanced\", random_state=0)\n",
    "elif model == \"LR\":\n",
    "    clf = LogisticRegression(\n",
    "        penalty=\"l1\", C=1, class_weight=\"balanced\", solver=\"saga\", random_state=0\n",
    "    )\n",
    "else:\n",
    "    print(f\"Unknown model: {model}\")\n",
    "\n",
    "print(f\"Using model: {model}\")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "train_acc = clf.score(X_train, y_train)\n",
    "print(f\"train acc: {train_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test set\n",
    "- accuracy\n",
    "- confusion_matrix\n",
    "- precision_recall_fscore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "print(f\"test acc: {test_acc:.3f}\")\n",
    "\n",
    "test_cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note the difference between train and test accuracy. If the difference is large, we are most likely overfitting the model to the train set. \n",
    "\n",
    "#### Things to try:\n",
    "- Increase regularization\n",
    "- Reduce dimensionality of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(font_scale=3)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    f, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "    g = sns.heatmap(test_cm, ax=ax, annot=True, annot_kws={\"fontsize\": 35}, cmap=\"Reds\")\n",
    "    g.set_title(\"Confusion matrix\")\n",
    "    g.set_ylabel(\"True label\")\n",
    "    g.set_xlabel(\"Pred label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "print(\n",
    "    f\"model: {model}, outcome: {outcome}\\n Acc:{test_acc:.2f}, precision: {p:.2f}, recall: {r:.2f}, f1: {f1:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's predict scanning site!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = \"SITE_ID\"\n",
    "y = pheno_df[outcome]\n",
    "y_counts = y.value_counts()\n",
    "\n",
    "print(f\"Unique output clasess:\\n{y_counts}\")\n",
    "\n",
    "# Encode labels to integer categories\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train-test split\n",
    "- 80/20 ratio\n",
    "- Stratify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_subset_fraction = 0.2  #\n",
    "stratification = y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,  # input features\n",
    "    y,  # output labels\n",
    "    test_size=test_subset_fraction,\n",
    "    shuffle=True,  # shuffle dataset\n",
    "    # before splitting\n",
    "    stratify=stratification,\n",
    "    random_state=123,  # same shuffle each time\n",
    ")\n",
    "\n",
    "# print the size of our training and test groups\n",
    "print(\"training:\", len(X_train), \"testing:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = \"LR\"  # 'LR' or 'RF'\n",
    "\n",
    "if model == \"RF\":\n",
    "    clf = RandomForestClassifier(max_depth=3, class_weight=\"balanced\", random_state=0)\n",
    "elif model == \"LR\":\n",
    "    clf = LogisticRegression(\n",
    "        penalty=\"l2\", C=1, class_weight=\"balanced\", solver=\"saga\", random_state=0\n",
    "    )\n",
    "else:\n",
    "    print(f\"Unknown model: {model}\")\n",
    "\n",
    "print(f\"Using model: {model}\")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "train_acc = clf.score(X_train, y_train)\n",
    "print(f\"train acc: {train_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test set\n",
    "- accuracy\n",
    "- confusion_matrix\n",
    "- precision_recall_fscore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "print(f\"test acc: {test_acc:.3f}\")\n",
    "\n",
    "test_cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(font_scale=3)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    f, ax = plt.subplots(figsize=(15, 10))\n",
    "    g = sns.heatmap(test_cm, annot=True, ax=ax, annot_kws={\"fontsize\": 40})\n",
    "    g.set_title(\"Confusion matrix\", fontsize=40)\n",
    "    g.set_ylabel(\"True label\", fontsize=40)\n",
    "    g.set_xlabel(\"Pred label\", fontsize=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "print(\n",
    "    f\"model: {model}, outcome: {outcome}\\n Acc:{test_acc:.2f}, precision: {p:.2f}, recall: {r:.2f}, f1: {f1:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5f8cee7ddba11edeefb1347c6536a4ac2b361bd4eba89a8b32d7cb85bbef9ea"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('green_compute': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
