{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's begin! \n",
    "### First we import some useful python libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "from nilearn import datasets\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subjects = 100\n",
    "parcel = \"rois_ho\"  # 'rois_ho' or 'rois_aal\n",
    "data = datasets.fetch_abide_pcp(n_subjects=n_subjects, derivatives=[parcel], data_dir=\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phenotypes and Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno = pd.DataFrame(data[\"phenotypic\"]).drop(columns=[\"i\", \"Unnamed: 0\"])\n",
    "pheno.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_counts = pheno[\"SITE_ID\"].value_counts()\n",
    "dx_counts = pheno[\"DX_GROUP\"].value_counts()\n",
    "\n",
    "print(f\"Dx count:\\n{dx_counts}\\n\\nScanning site_counts:\\n{site_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRI features\n",
    "\n",
    "### These are stored in a list, where each list element is a subject-specific feature matrix\n",
    "### Subject specific feature matrix: timepoints x ROIs\n",
    "### ROIs are defined based on [harvard_oxford atlas](https://nilearn.github.io/modules/generated/nilearn.datasets.fetch_atlas_harvard_oxford.html) or [AAL atlas](https://nilearn.github.io/modules/generated/nilearn.datasets.fetch_atlas_aal.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[parcel]\n",
    "\n",
    "print(f\"Number of samples: {len(features)}\")\n",
    "\n",
    "subject_feature_shape = features[0].shape\n",
    "\n",
    "print(f\"subject_feature_shape: {subject_feature_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how the atlas looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "\n",
    "if parcel == \"rois_ho\":\n",
    "    atlas = datasets.fetch_atlas_harvard_oxford(\"cort-maxprob-thr25-2mm\")\n",
    "else:\n",
    "    atlas = datasets.fetch_atlas_aal()\n",
    "\n",
    "plotting.plot_roi(atlas.maps, draw_cross=False, title=parcel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the subject-specific feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "g = sns.heatmap(features[0].T, ax=ax)\n",
    "g.set_xlabel(\"timepoint\")\n",
    "g.set_ylabel(\"ROI\")\n",
    "g.set_title(\"Functional data timeseries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing / feature engineering\n",
    "\n",
    "### Commonly functional (timeseries) neuroimaging data is represented as functional connectome aka network ake graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectome_matrix = ConnectivityMeasure(kind=\"correlation\")\n",
    "connectome_matrix = connectome_matrix.fit_transform([features[0]])[0]\n",
    "print(f\"Shape connectome: {connectome_matrix.shape}\")\n",
    "\n",
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "g = sns.heatmap(connectome_matrix, ax=ax)\n",
    "g.set_xlabel(\"ROI\")\n",
    "g.set_ylabel(\"ROI\")\n",
    "g.set_title(\"Connectome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract lower (or upper) triangle entrees (excluding diagonal)\n",
    "tril_idx = np.tril_indices(len(connectome_matrix), k=-1)\n",
    "features_flat = connectome_matrix[tril_idx]\n",
    "print(f\"Number of features per subject: {len(features_flat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now do this for all subjects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defs are definitely useful!\n",
    "\n",
    "\n",
    "def extract_connectome_features(func_data, measure):\n",
    "    \"\"\"A function to calculate connnectome based on timeseries data and similarity measure\"\"\"\n",
    "    connectome_matrix = measure.fit_transform([func_data])[0]\n",
    "    tril_idx = np.tril_indices(len(connectome_matrix), k=-1)\n",
    "    flat_features = connectome_matrix[tril_idx]\n",
    "\n",
    "    return flat_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "here we are pre-processing each image independently i.e. not using any group-level information for scaling / normalization / feature transformation (e.g. PCA). Therefore there is no \"double dipping\" or leakage of information from a test set. This sort of independent image pre-processing, we can do on entire dataset without creating train-test splits first and then defining feature-set on the training data only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_measure = ConnectivityMeasure(kind=\"correlation\")\n",
    "\n",
    "flat_features_list = []\n",
    "for func_data in features:\n",
    "    flat_features = extract_connectome_features(func_data, correlation_measure)\n",
    "    flat_features_list.append(flat_features)\n",
    "\n",
    "print(f\"Length of flat_features_list {len(flat_features_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data matrix (n_samples, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(flat_features_list)\n",
    "\n",
    "print(f\"Input data (X) shape: {X.shape}\")\n",
    "\n",
    "# Memory intensive #\n",
    "\n",
    "# g = sns.heatmap(flat_features_array)\n",
    "# g.set_xlabel('Connection strength')\n",
    "# g.set_ylabel('ROI')\n",
    "# g.set_title('Connectome')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output labels (y): Diagnosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "outcome = \"DX_GROUP\"\n",
    "y = pheno[outcome]\n",
    "y_counts = y.value_counts()\n",
    "\n",
    "print(f\"Unique output clasess:\\n{y_counts}\")\n",
    "\n",
    "# Encode labels to integer categories\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okay now we have our input data (X) and output data (y) in the following format\n",
    "<img src=\"./QLS_ML_terminology.png\" alt=\"terms\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save `ML-ready` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset = False\n",
    "\n",
    "ds_name = \"my_ML_dataset\"\n",
    "save_path = f\"../../data/{ds_name}\"\n",
    "\n",
    "if save_dataset:\n",
    "    print(f\"Saving dataset to {save_path}\")\n",
    "    print(f\"Dataset X_shape: {X.shape}, y_shape: {y.shape}\")\n",
    "    np.savez(save_path, X=X, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset (sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading dataset from: {save_path}\")\n",
    "my_ds = np.load(save_path + \".npz\")\n",
    "X = my_ds[\"X\"]\n",
    "y = my_ds[\"y\"]\n",
    "print(f\"Loaded dataset X_shape: {X.shape}, y_shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5f8cee7ddba11edeefb1347c6536a4ac2b361bd4eba89a8b32d7cb85bbef9ea"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('green_compute': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
